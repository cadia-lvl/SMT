<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>frontend.core API documentation</title>
<meta name="description" content="Sentence level processing. Some useful functions to process sentences." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>frontend.core</code></h1>
</header>
<section id="section-intro">
<p>Sentence level processing. Some useful functions to process sentences.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Sentence level processing. Some useful functions to process sentences.
&#34;&#34;&#34;
import re
from unicodedata import normalize
from typing import Tuple, List, Dict, Callable, \
    Sequence
from enum import Enum
from functools import partial

import tokenizer as mideind_tok
import nltk
# also in sacremoses: punct norm, detok and sent split
from sacremoses import MosesTokenizer

nltk.download(&#39;punkt&#39;)


class Lang(Enum):
    &#34;&#34;&#34;An enum for supported ISO 639-1 language codes. String based&#34;&#34;&#34;
    EN = &#39;en&#39;
    IS = &#39;is&#39;


REGEXP_SUB: Dict[str, Tuple[re.Pattern, str]] = {
    # Taken from https://stackoverflow.com/questions/3809401/what-is-a-good-regular-expression-to-match-a-url?noredirect=1&amp;lq=1
    &#39;URI-OLD&#39;: (
        re.compile(
            r&#39;(http(s)?:\/\/.)?(www\.)?[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}\b([-a-zA-Z0-9@:%_\+.~#?&amp;//=]*)&#39;),
        &#39;@uri@&#39;),
    &#39;IS-COMBINE-NEWLINE&#39;: (re.compile(r&#39;([\w]+)\.\n([a-záðéíóúýþæö])&#39;),
                           r&#39;\1. \2&#39;),
    &#39;IS-SPLIT-NEWLINE&#39;: (re.compile(r&#39;([\w\(\)\[\]\.]{2,})\.([A-ZÁÐÉÍÓÚÝÞÆÖ])&#39;),
                         r&#39;\1. \2&#39;),
    &#39;URI&#39;: (re.compile(
        r&#34;((http(s)?:\/\/)|(www)|([-a-zA-Z0-9:%_\+.~#?&amp;/=]+?@))+([-a-zA-Z0-9@:%_\+.~#?&amp;/=]+)&#34;),
            &#39;@uri@&#39;),
    &#39;URI-SIMPLE&#39;: (re.compile(r&#34;([-a-zA-Z0-9@:%_\+.~#?&amp;/=]+?)(\.is|\.com)&#34;), &#34;@uri@&#34;),
    &#39;EMPTY-BRACKETS&#39;: (re.compile(r&#34;[\[\(]\s*[\]\)]&#34;), &#34;&#34;),
    # u&#39;\u007c&#39; - |
    &#39;PIPE&#39;: (re.compile(r&#34;\u007c&#34;), &#39;@pipe@&#39;),
    # u&#39;\u003c&#39;, u&#39;\u003e&#39; - &lt;, &gt;
    &#39;LT&#39;: (re.compile(r&#34;\u003c&#34;), &#39;@lt@&#39;),
    &#39;GT&#39;: (re.compile(r&#34;\u003e&#34;), &#39;@gt@&#39;),
    # u&#39;\u005b&#39;, u&#39;\u005d&#39; - [, ]
    &#39;BRACKET-OPEN&#39;: (re.compile(r&#34;\u005b&#34;), &#39;@brac_open@&#39;),
    &#39;BRACKET-CLOSE&#39;: (re.compile(r&#34;\u005d&#34;), &#39;@brac_close@&#39;),
    &#39;FIX-URI&#39;: (re.compile(r&#34;@ uri @&#34;), &#39;@uri@&#39;),
    &#39;CRYLLIC&#39;: (re.compile(r&#39;.*[\u0400-\u04FF\u0500-\u052F\u2DE0-\u2DFF\uA640-\uA69F]+.*&#39;),
                &#39;&#39;),
    &#39;GREEK&#39;: (re.compile(r&#39;.*[\u0370-\u03bb\u03bd-\u03FF\u1F00-\u1FFF]+.*&#39;), &#39;&#39;),
    &#39;UNKNOWN-CHARS&#39;: (re.compile(r&#39;.*[žčšè¿ğūįł]+.*&#39;), &#39;&#39;),
    &#39;NOT-WORDS&#39;: (re.compile(r&#39;.*[\W\d_].*&#39;), &#39;&#39;),
    &#39;NOT-WORDS-OLD&#39;: (re.compile(r&#39;.*[\d();.:,•\-=?!_+@].*&#39;), &#39;&#39;)
}


def regexp(sent: str, regexps: List[Tuple[re.Pattern, str]]) -&gt; str:
    &#34;&#34;&#34;Applies a list of regular expressions and their substitutions to a string.

    :param sent: The sentence to process.\n
    :param regexps: A list of Tuples (re.Pattern, str).\n
    The pattern is used to match and the str is used as a replacement.
    The str supports referencing groups in the match expression.\n
    :return: The processed sentence.
    &#34;&#34;&#34;
    processed_line = sent
    for regular_expression, sub_string in regexps:
        processed_line = re.sub(
            regular_expression, sub_string, processed_line)
    return processed_line


def lowercase_normalize(sent: str) -&gt; str:
    &#34;&#34;&#34;Applies unicode lowercase and normalize (NFKC) on a string.

    :param sent: The sentence to process.\n
    :return: The processed sentence.
    &#34;&#34;&#34;
    return normalize(&#39;NFKC&#39;, sent.casefold())


def get_tokenizer(lang: Lang, method: str) -&gt; Callable[[str], List[str]]:
    &#34;&#34;&#34;Returns a tokenizer for a specified method and additional arguments.

    Supported methods:

    - IS(default): &#34;pass-through&#34;, basic tokenization.
    - IS: &#34;placeholders&#34;, uses placeholders for some NEs.
    - IS: &#34;moses&#34;, uses Moses tokenization. Poor performance for IS.
    - EN(default): &#34;moses&#34;, Moses tokenization, splits up URLs. Poor abbreviation handling.
    - EN: &#34;nltk&#34;, splits up URLs.
    - EN: &#34;toktok&#34;, handles URLs, does not handle &#34;.&#34; but at the end.
    &#34;&#34;&#34;
    if lang == Lang.EN:
        if method == &#34;nltk&#34;:
            # We use the word_tokenize NLTL tokenizer for english
            return nltk.word_tokenize
        # o.w. we use Moses
        if method == &#39;toktok&#39;:
            toktok = nltk.tokenize.ToktokTokenizer()
            return toktok.tokenize
        m_tok = MosesTokenizer(lang=&#39;en&#39;)
        return partial(m_tok.tokenize, escape=False)
    # Moses for &#39;is&#39;
    if method == &#39;moses&#39;:
        m_tok = MosesTokenizer(lang=&#39;is&#39;)
        return partial(m_tok.tokenize, escape=False)
    return partial(_tokenize_is, method=method)


def apply_tokenizer(sentence: str, tokenizer: Callable[[str], List[str]]) -&gt; str:
    &#34;&#34;&#34;Applies a tokenization function to a sentence.

    :param sentence: The sentence to process.\n
    :param tokenizer: The tokenizer function to use.\n
    :return: The processed sentence.
    &#34;&#34;&#34;
    return &#34; &#34;.join(tokenizer(sentence)) + &#34;\n&#34;


def tokenize(sentence: str, lang: Lang, method: str = &#39;pass-through&#39;):
    &#34;&#34;&#34;Tokenizes a sentence using the specified method. See get_tokenizer() for supported methods.

    :param sentence: The sentence to process.\n
    :param lang: The language of the sentence.\n
    :param method: The tokenization method to use. See get_tokenizer() for supported methods.\n
    :return: The processed sentence.

    &#34;&#34;&#34;
    tok = get_tokenizer(lang, method)
    return apply_tokenizer(sentence, tok)


def _tokenize_is(sentence: str, method: str):
    &#34;&#34;&#34;Helper function. Tokenizes an Icelandic sentence.&#34;&#34;&#34;
    # We set the option to change &#34;1sti&#34;, ... to &#34;1&#34;, ...
    result = []
    for token in mideind_tok.tokenize(sentence,
                                      handle_kludgy_ordinals=mideind_tok.KLUDGY_ORDINALS_MODIFY):
        kind, txt, val = token
        if method == &#39;pass-through&#39;:
            token = _tok_pass_through(kind, txt, val)
        elif method == &#39;placeholders&#39;:
            token = _tok_placeholders(kind, txt, val)
        if token:
            result.append(token)
    return result


def _tok_pass_through(kind, txt, val):
    if kind == mideind_tok.TOK.WORD:
        if val:
            return val[0][0]
        return txt
    if kind == mideind_tok.TOK.PERCENT:
        return f&#39;{val[0]} %&#39;
    if kind == mideind_tok.TOK.S_BEGIN:
        return None
    if kind == mideind_tok.TOK.S_END:
        return None
    return txt


def _tok_placeholders(kind, txt, val):
    if kind == mideind_tok.TOK.WORD:
        if val:
            return val[0][0]
        return txt
    if kind == mideind_tok.TOK.ORDINAL:
        return &#34;TALA&#34;
    if kind == mideind_tok.TOK.NUMBER:
        return &#34;NÚMER&#34;
    if kind == mideind_tok.TOK.PUNCTUATION:
        return txt
    if kind == mideind_tok.TOK.YEAR:
        return &#34;ÁR&#34;
    if kind == mideind_tok.TOK.S_BEGIN:
        pass
    if kind == mideind_tok.TOK.S_END:
        pass
    if kind == mideind_tok.TOK.DATEABS:
        return &#34;DAGSETNING&#34;
    if kind == mideind_tok.TOK.DATEREL:
        return &#34;DAGSETNING&#34;
    if kind == mideind_tok.TOK.MEASUREMENT:
        return &#34;MÆLING&#34;
    if kind == mideind_tok.TOK.NUMWLETTER:
        return &#34;GILDI&#34;
    if kind == mideind_tok.TOK.DOMAIN:
        return &#34;LÉN&#34;
    if kind == mideind_tok.TOK.HASHTAG:
        return &#34;HASHTAG&#34;
    if kind == mideind_tok.TOK.TELNO:
        return &#34;SÍMANÚMER&#34;
    if kind == mideind_tok.TOK.PERCENT:
        return &#34;PRÓSENTA&#34;
    if kind == mideind_tok.TOK.URL:
        return &#34;VEFFANG&#34;
    if kind == mideind_tok.TOK.AMOUNT:
        return &#34;UPPHÆÐ&#34;
    if kind == mideind_tok.TOK.EMAIL:
        return &#34;TÖLVUPÓSTUR&#34;
    if kind == mideind_tok.TOK.UNKNOWN:
        return &#34;UNKOWN&#34;
    return &#34;UNKOWN&#34;


def known_tok_fraction(sentence: str, known_tokens: Sequence[str]) -&gt; float:
    &#34;&#34;&#34;Returns the fraction of known words in the sentence. Works best if the sentence has been tokenized and
    if the sentence has been normalized to only words.

    :param sentence: The sentence to processes.\n
    :param known_tokens: A Sequence of known tokens to compare to.\n
    :return: The fraction of known tokens in sentence according to &#34;known_tokens&#34;.
    &#34;&#34;&#34;
    sent_tokens = sentence.split()
    known = 0
    token_count = len(sent_tokens)
    for token in sent_tokens:
        if token in known_tokens:
            known += 1
    return known / token_count


def contains_regexp(sentence: str, regexp: re.Pattern) -&gt; bool:
    &#34;&#34;&#34;Checks if the sentence contains the regexp.

    :param sentence: The sentence to process.
    :param regexp: The regular expression to search for.
    :return: True if the regular expression is found in the sentence.
    &#34;&#34;&#34;
    if re.match(regexp, sentence) is None:
        return False
    return True


def remove_non_words(sentence: str) -&gt; str:
    &#34;&#34;&#34;Normalizes a sentence by removing all words which contain punctuation, numbers and other non-word symbols.

    :param sentence: The sentence to process.\n
    :return: The processed sentence.
    &#34;&#34;&#34;
    result = []
    tokens = sentence.split()
    for token in tokens:
        if not contains_regexp(token, REGEXP_SUB[&#39;NOT-WORDS&#39;][0]):
            result.append(token)
    return &#34; &#34;.join(result)


def should_drop(line: str,
                regexps: Sequence[re.Pattern],
                known_tokens: Sequence[str],
                keep_ratio=0.5,
                normalize=True,
                keep_sent_length=1) -&gt; Tuple[bool, float, str]:
    &#34;&#34;&#34;Decides if a line should be dropped given the criteria.
    Regexp defines a black-list of regular expressions.

    If normalized=True all non-words are removed from the sentence before counting the numer of words.\n
    If the remaining sentence contains any of the regexps it is DROPPED.\n
    If the remaining sentence has length less than or equal to keep_sent_length is it KEPT.\n
    If the keep_ratio is smaller or equal to the fraction of known_tokens in sentence it is KEPT.

    :param line: The sentence to process.\n
    :param regexps: A black-list of regular expressions. If any is matched in a sentence, it is DROPPED.\n
    :param known_tokens: A whitelist of tokens which are considered as known.\n
    :param keep_ratio: If the fraction of known tokens is higher than keep_ration, the sentence is KEPT.\n
    :param normalize: If True, we first normalize the sentence by removing all words which contain non-words.\n
    :param keep_sent_length: If a sentence contains keep_sent_length of fewer words, it is KEPT.\n
    :return: A Tuple (Should drop, the known fraction, the line).\n
    &#34;&#34;&#34;
    normalized_line = line
    if normalize:
        # We normalize the tokens in the sentence, by only considering words
        normalized_line = remove_non_words(line)
    if not normalized_line:
        return True, 0.0, line
    for regexp in regexps:
        if contains_regexp(normalized_line, regexp):
            return True, 0.0, line
    # we want sentences which have a minimum length
    if len(normalized_line.split()) &lt;= keep_sent_length:
        return False, 1.0, line
    fraction = known_tok_fraction(normalized_line, known_tokens)
    # We keep lines which have a high fraction
    if keep_ratio &lt;= fraction:
        return False, fraction, line
    return True, fraction, line</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="frontend.core.apply_tokenizer"><code class="name flex">
<span>def <span class="ident">apply_tokenizer</span></span>(<span>sentence, tokenizer)</span>
</code></dt>
<dd>
<section class="desc"><p>Applies a tokenization function to a sentence.</p>
<p>:param sentence: The sentence to process.</p>
<p>:param tokenizer: The tokenizer function to use.</p>
<p>:return: The processed sentence.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_tokenizer(sentence: str, tokenizer: Callable[[str], List[str]]) -&gt; str:
    &#34;&#34;&#34;Applies a tokenization function to a sentence.

    :param sentence: The sentence to process.\n
    :param tokenizer: The tokenizer function to use.\n
    :return: The processed sentence.
    &#34;&#34;&#34;
    return &#34; &#34;.join(tokenizer(sentence)) + &#34;\n&#34;</code></pre>
</details>
</dd>
<dt id="frontend.core.contains_regexp"><code class="name flex">
<span>def <span class="ident">contains_regexp</span></span>(<span>sentence, regexp)</span>
</code></dt>
<dd>
<section class="desc"><p>Checks if the sentence contains the regexp.</p>
<p>:param sentence: The sentence to process.
:param regexp: The regular expression to search for.
:return: True if the regular expression is found in the sentence.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def contains_regexp(sentence: str, regexp: re.Pattern) -&gt; bool:
    &#34;&#34;&#34;Checks if the sentence contains the regexp.

    :param sentence: The sentence to process.
    :param regexp: The regular expression to search for.
    :return: True if the regular expression is found in the sentence.
    &#34;&#34;&#34;
    if re.match(regexp, sentence) is None:
        return False
    return True</code></pre>
</details>
</dd>
<dt id="frontend.core.get_tokenizer"><code class="name flex">
<span>def <span class="ident">get_tokenizer</span></span>(<span>lang, method)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns a tokenizer for a specified method and additional arguments.</p>
<p>Supported methods:</p>
<ul>
<li>IS(default): "pass-through", basic tokenization.</li>
<li>IS: "placeholders", uses placeholders for some NEs.</li>
<li>IS: "moses", uses Moses tokenization. Poor performance for IS.</li>
<li>EN(default): "moses", Moses tokenization, splits up URLs. Poor abbreviation handling.</li>
<li>EN: "nltk", splits up URLs.</li>
<li>EN: "toktok", handles URLs, does not handle "." but at the end.</li>
</ul></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tokenizer(lang: Lang, method: str) -&gt; Callable[[str], List[str]]:
    &#34;&#34;&#34;Returns a tokenizer for a specified method and additional arguments.

    Supported methods:

    - IS(default): &#34;pass-through&#34;, basic tokenization.
    - IS: &#34;placeholders&#34;, uses placeholders for some NEs.
    - IS: &#34;moses&#34;, uses Moses tokenization. Poor performance for IS.
    - EN(default): &#34;moses&#34;, Moses tokenization, splits up URLs. Poor abbreviation handling.
    - EN: &#34;nltk&#34;, splits up URLs.
    - EN: &#34;toktok&#34;, handles URLs, does not handle &#34;.&#34; but at the end.
    &#34;&#34;&#34;
    if lang == Lang.EN:
        if method == &#34;nltk&#34;:
            # We use the word_tokenize NLTL tokenizer for english
            return nltk.word_tokenize
        # o.w. we use Moses
        if method == &#39;toktok&#39;:
            toktok = nltk.tokenize.ToktokTokenizer()
            return toktok.tokenize
        m_tok = MosesTokenizer(lang=&#39;en&#39;)
        return partial(m_tok.tokenize, escape=False)
    # Moses for &#39;is&#39;
    if method == &#39;moses&#39;:
        m_tok = MosesTokenizer(lang=&#39;is&#39;)
        return partial(m_tok.tokenize, escape=False)
    return partial(_tokenize_is, method=method)</code></pre>
</details>
</dd>
<dt id="frontend.core.known_tok_fraction"><code class="name flex">
<span>def <span class="ident">known_tok_fraction</span></span>(<span>sentence, known_tokens)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the fraction of known words in the sentence. Works best if the sentence has been tokenized and
if the sentence has been normalized to only words.</p>
<p>:param sentence: The sentence to processes.</p>
<p>:param known_tokens: A Sequence of known tokens to compare to.</p>
<p>:return: The fraction of known tokens in sentence according to "known_tokens".</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def known_tok_fraction(sentence: str, known_tokens: Sequence[str]) -&gt; float:
    &#34;&#34;&#34;Returns the fraction of known words in the sentence. Works best if the sentence has been tokenized and
    if the sentence has been normalized to only words.

    :param sentence: The sentence to processes.\n
    :param known_tokens: A Sequence of known tokens to compare to.\n
    :return: The fraction of known tokens in sentence according to &#34;known_tokens&#34;.
    &#34;&#34;&#34;
    sent_tokens = sentence.split()
    known = 0
    token_count = len(sent_tokens)
    for token in sent_tokens:
        if token in known_tokens:
            known += 1
    return known / token_count</code></pre>
</details>
</dd>
<dt id="frontend.core.lowercase_normalize"><code class="name flex">
<span>def <span class="ident">lowercase_normalize</span></span>(<span>sent)</span>
</code></dt>
<dd>
<section class="desc"><p>Applies unicode lowercase and normalize (NFKC) on a string.</p>
<p>:param sent: The sentence to process.</p>
<p>:return: The processed sentence.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lowercase_normalize(sent: str) -&gt; str:
    &#34;&#34;&#34;Applies unicode lowercase and normalize (NFKC) on a string.

    :param sent: The sentence to process.\n
    :return: The processed sentence.
    &#34;&#34;&#34;
    return normalize(&#39;NFKC&#39;, sent.casefold())</code></pre>
</details>
</dd>
<dt id="frontend.core.regexp"><code class="name flex">
<span>def <span class="ident">regexp</span></span>(<span>sent, regexps)</span>
</code></dt>
<dd>
<section class="desc"><p>Applies a list of regular expressions and their substitutions to a string.</p>
<p>:param sent: The sentence to process.</p>
<p>:param regexps: A list of Tuples (re.Pattern, str).</p>
<p>The pattern is used to match and the str is used as a replacement.
The str supports referencing groups in the match expression.</p>
<p>:return: The processed sentence.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def regexp(sent: str, regexps: List[Tuple[re.Pattern, str]]) -&gt; str:
    &#34;&#34;&#34;Applies a list of regular expressions and their substitutions to a string.

    :param sent: The sentence to process.\n
    :param regexps: A list of Tuples (re.Pattern, str).\n
    The pattern is used to match and the str is used as a replacement.
    The str supports referencing groups in the match expression.\n
    :return: The processed sentence.
    &#34;&#34;&#34;
    processed_line = sent
    for regular_expression, sub_string in regexps:
        processed_line = re.sub(
            regular_expression, sub_string, processed_line)
    return processed_line</code></pre>
</details>
</dd>
<dt id="frontend.core.remove_non_words"><code class="name flex">
<span>def <span class="ident">remove_non_words</span></span>(<span>sentence)</span>
</code></dt>
<dd>
<section class="desc"><p>Normalizes a sentence by removing all words which contain punctuation, numbers and other non-word symbols.</p>
<p>:param sentence: The sentence to process.</p>
<p>:return: The processed sentence.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_non_words(sentence: str) -&gt; str:
    &#34;&#34;&#34;Normalizes a sentence by removing all words which contain punctuation, numbers and other non-word symbols.

    :param sentence: The sentence to process.\n
    :return: The processed sentence.
    &#34;&#34;&#34;
    result = []
    tokens = sentence.split()
    for token in tokens:
        if not contains_regexp(token, REGEXP_SUB[&#39;NOT-WORDS&#39;][0]):
            result.append(token)
    return &#34; &#34;.join(result)</code></pre>
</details>
</dd>
<dt id="frontend.core.should_drop"><code class="name flex">
<span>def <span class="ident">should_drop</span></span>(<span>line, regexps, known_tokens, keep_ratio=0.5, normalize=True, keep_sent_length=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Decides if a line should be dropped given the criteria.
Regexp defines a black-list of regular expressions.</p>
<p>If normalized=True all non-words are removed from the sentence before counting the numer of words.</p>
<p>If the remaining sentence contains any of the regexps it is DROPPED.</p>
<p>If the remaining sentence has length less than or equal to keep_sent_length is it KEPT.</p>
<p>If the keep_ratio is smaller or equal to the fraction of known_tokens in sentence it is KEPT.</p>
<p>:param line: The sentence to process.</p>
<p>:param regexps: A black-list of regular expressions. If any is matched in a sentence, it is DROPPED.</p>
<p>:param known_tokens: A whitelist of tokens which are considered as known.</p>
<p>:param keep_ratio: If the fraction of known tokens is higher than keep_ration, the sentence is KEPT.</p>
<p>:param normalize: If True, we first normalize the sentence by removing all words which contain non-words.</p>
<p>:param keep_sent_length: If a sentence contains keep_sent_length of fewer words, it is KEPT.</p>
<p>:return: A Tuple (Should drop, the known fraction, the line).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def should_drop(line: str,
                regexps: Sequence[re.Pattern],
                known_tokens: Sequence[str],
                keep_ratio=0.5,
                normalize=True,
                keep_sent_length=1) -&gt; Tuple[bool, float, str]:
    &#34;&#34;&#34;Decides if a line should be dropped given the criteria.
    Regexp defines a black-list of regular expressions.

    If normalized=True all non-words are removed from the sentence before counting the numer of words.\n
    If the remaining sentence contains any of the regexps it is DROPPED.\n
    If the remaining sentence has length less than or equal to keep_sent_length is it KEPT.\n
    If the keep_ratio is smaller or equal to the fraction of known_tokens in sentence it is KEPT.

    :param line: The sentence to process.\n
    :param regexps: A black-list of regular expressions. If any is matched in a sentence, it is DROPPED.\n
    :param known_tokens: A whitelist of tokens which are considered as known.\n
    :param keep_ratio: If the fraction of known tokens is higher than keep_ration, the sentence is KEPT.\n
    :param normalize: If True, we first normalize the sentence by removing all words which contain non-words.\n
    :param keep_sent_length: If a sentence contains keep_sent_length of fewer words, it is KEPT.\n
    :return: A Tuple (Should drop, the known fraction, the line).\n
    &#34;&#34;&#34;
    normalized_line = line
    if normalize:
        # We normalize the tokens in the sentence, by only considering words
        normalized_line = remove_non_words(line)
    if not normalized_line:
        return True, 0.0, line
    for regexp in regexps:
        if contains_regexp(normalized_line, regexp):
            return True, 0.0, line
    # we want sentences which have a minimum length
    if len(normalized_line.split()) &lt;= keep_sent_length:
        return False, 1.0, line
    fraction = known_tok_fraction(normalized_line, known_tokens)
    # We keep lines which have a high fraction
    if keep_ratio &lt;= fraction:
        return False, fraction, line
    return True, fraction, line</code></pre>
</details>
</dd>
<dt id="frontend.core.tokenize"><code class="name flex">
<span>def <span class="ident">tokenize</span></span>(<span>sentence, lang, method='pass-through')</span>
</code></dt>
<dd>
<section class="desc"><p>Tokenizes a sentence using the specified method. See get_tokenizer() for supported methods.</p>
<p>:param sentence: The sentence to process.</p>
<p>:param lang: The language of the sentence.</p>
<p>:param method: The tokenization method to use. See get_tokenizer() for supported methods.</p>
<p>:return: The processed sentence.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tokenize(sentence: str, lang: Lang, method: str = &#39;pass-through&#39;):
    &#34;&#34;&#34;Tokenizes a sentence using the specified method. See get_tokenizer() for supported methods.

    :param sentence: The sentence to process.\n
    :param lang: The language of the sentence.\n
    :param method: The tokenization method to use. See get_tokenizer() for supported methods.\n
    :return: The processed sentence.

    &#34;&#34;&#34;
    tok = get_tokenizer(lang, method)
    return apply_tokenizer(sentence, tok)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="frontend.core.Lang"><code class="flex name class">
<span>class <span class="ident">Lang</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>An enum for supported ISO 639-1 language codes. String based</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Lang(Enum):
    &#34;&#34;&#34;An enum for supported ISO 639-1 language codes. String based&#34;&#34;&#34;
    EN = &#39;en&#39;
    IS = &#39;is&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="frontend.core.Lang.EN"><code class="name">var <span class="ident">EN</span></code></dt>
<dd>
<section class="desc"><p>An enum for supported ISO 639-1 language codes. String based</p></section>
</dd>
<dt id="frontend.core.Lang.IS"><code class="name">var <span class="ident">IS</span></code></dt>
<dd>
<section class="desc"><p>An enum for supported ISO 639-1 language codes. String based</p></section>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="frontend" href="index.html">frontend</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="frontend.core.apply_tokenizer" href="#frontend.core.apply_tokenizer">apply_tokenizer</a></code></li>
<li><code><a title="frontend.core.contains_regexp" href="#frontend.core.contains_regexp">contains_regexp</a></code></li>
<li><code><a title="frontend.core.get_tokenizer" href="#frontend.core.get_tokenizer">get_tokenizer</a></code></li>
<li><code><a title="frontend.core.known_tok_fraction" href="#frontend.core.known_tok_fraction">known_tok_fraction</a></code></li>
<li><code><a title="frontend.core.lowercase_normalize" href="#frontend.core.lowercase_normalize">lowercase_normalize</a></code></li>
<li><code><a title="frontend.core.regexp" href="#frontend.core.regexp">regexp</a></code></li>
<li><code><a title="frontend.core.remove_non_words" href="#frontend.core.remove_non_words">remove_non_words</a></code></li>
<li><code><a title="frontend.core.should_drop" href="#frontend.core.should_drop">should_drop</a></code></li>
<li><code><a title="frontend.core.tokenize" href="#frontend.core.tokenize">tokenize</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="frontend.core.Lang" href="#frontend.core.Lang">Lang</a></code></h4>
<ul class="">
<li><code><a title="frontend.core.Lang.EN" href="#frontend.core.Lang.EN">EN</a></code></li>
<li><code><a title="frontend.core.Lang.IS" href="#frontend.core.Lang.IS">IS</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>