{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moses IS-EN EN-IS phrase þýðingarvél\n",
    "Sjá `README.md` til þess að keyra þetta vélrit (e. notebook).\n",
    "\n",
    "Í þessu vélriti eru gögn forunnin og Moses þýðingarkerfið notað til þess búa til tvö þýðingarkerfi, IS-EN og EN-IS.\n",
    "Það er gert ráð fyrir því að öll gögn séu aðgengileg undir `/work/data`. Sjá leiðbeiningar í `README.md` um hvernig það er gert með `docker` eða `singularity`.\n",
    "\n",
    "Í stuttu máli skiptist vélritið í eftirfarandi þætti:\n",
    "1. Samhliða og einhliða gögn undirbúin.\n",
    "1. Tungumála módel byggt fyrir EN og IS (KenLM).\n",
    "1. Texta skipt í þrjá hluta; train/val/test, fjöldi setninga í val/test er 3000/2000.\n",
    "1. Moses kerfið þjálfað með train hluta texta.\n",
    "1. Moses kerfið fínpússað með val hluta texta.\n",
    "1. Moses kerfið metið með BLEU mælingin á test hluta texta.\n",
    "\n",
    "Allar skrár og líkön eru raðað í skrána \"WORKING_DIR\" (sjá `README.md`).\n",
    "\n",
    "Safnið `corpus.py` skilgreinir föll og gagnategundir sem eru mikið nýttar hér."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/staff/haukurpj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter, OrderedDict\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import re\n",
    "from pprint import pprint\n",
    "import importlib\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import corpus.corpus as c\n",
    "\n",
    "importlib.reload(c)\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "working_dir = pathlib.Path('/work')\n",
    "data_dir = working_dir.joinpath('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be sure that Moses is installed and the data is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/moses\n",
      "/opt/moses_tools\n",
      "10\n",
      "bin  en-monolingual.zip  parice  risamalheild\r\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv('MOSESDECODER'))\n",
    "print(os.getenv('MOSESDECODER_TOOLS'))\n",
    "print(int(os.getenv('THREADS')))\n",
    "!ls {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bpe': None,\n",
      " 'bpe-cat': PosixPath('/work/data/risamalheild/bpe-cat.is'),\n",
      " 'bpe-final': PosixPath('/work/data/risamalheild/bpe-final.is'),\n",
      " 'bpe-lm-blm': PosixPath('/work/data/risamalheild/bpe-lm-blm.is'),\n",
      " 'cat': PosixPath('/work/data/risamalheild/cat.is'),\n",
      " 'final': PosixPath('/work/data/risamalheild/final.is'),\n",
      " 'kvistur': PosixPath('/work/data/risamalheild/kvistur.is'),\n",
      " 'kvistur-cat': PosixPath('/work/data/risamalheild/kvistur-cat.is'),\n",
      " 'kvistur-final': PosixPath('/work/data/risamalheild/kvistur-final.is'),\n",
      " 'kvistur-lm-blm': PosixPath('/work/data/risamalheild/kvistur-lm-blm.is'),\n",
      " 'lm-blm': PosixPath('/work/data/risamalheild/lm-blm.is'),\n",
      " 'lower': None,\n",
      " 'placeholders': None,\n",
      " 'regexp': None,\n",
      " 'sent_fix': None,\n",
      " 'tok': None}\n",
      "{'bpe': PosixPath('/work/data/parice/train/bpe.en'),\n",
      " 'bpe-final': PosixPath('/work/data/parice/train/bpe-final.en'),\n",
      " 'bpe-length': PosixPath('/work/data/parice/train/bpe-length.en'),\n",
      " 'bpe-lm-blm': PosixPath('/work/data/parice/train/bpe-lm-blm.en'),\n",
      " 'drop': PosixPath('/work/data/parice/train/drop.en'),\n",
      " 'final': PosixPath('/work/data/parice/train/final.en'),\n",
      " 'kvistur': None,\n",
      " 'kvistur-final': PosixPath('/work/data/parice/train/kvistur-final.en'),\n",
      " 'kvistur-length': PosixPath('/work/data/parice/train/kvistur-length.en'),\n",
      " 'kvistur-lm-blm': None,\n",
      " 'length': PosixPath('/work/data/parice/train/length.en'),\n",
      " 'lm-blm': PosixPath('/work/data/parice/train/lm-blm.en'),\n",
      " 'lower': None,\n",
      " 'placeholders': None,\n",
      " 'regexp': PosixPath('/work/data/parice/train/regexp.en'),\n",
      " 'shuffle': None,\n",
      " 'tok': None,\n",
      " 'translated_en_is': None,\n",
      " 'translated_is_en': None}\n",
      "{'bpe': PosixPath('/work/data/parice/train/bpe.is'),\n",
      " 'bpe-final': PosixPath('/work/data/parice/train/bpe-final.is'),\n",
      " 'bpe-length': PosixPath('/work/data/parice/train/bpe-length.is'),\n",
      " 'bpe-lm-blm': None,\n",
      " 'drop': PosixPath('/work/data/parice/train/drop.is'),\n",
      " 'final': PosixPath('/work/data/parice/train/final.is'),\n",
      " 'kvistur': PosixPath('/work/data/parice/train/kvistur.is'),\n",
      " 'kvistur-final': PosixPath('/work/data/parice/train/kvistur-final.is'),\n",
      " 'kvistur-length': PosixPath('/work/data/parice/train/kvistur-length.is'),\n",
      " 'kvistur-lm-blm': None,\n",
      " 'length': PosixPath('/work/data/parice/train/length.is'),\n",
      " 'lm-blm': PosixPath('/work/data/parice/train/lm-blm.is'),\n",
      " 'lower': None,\n",
      " 'placeholders': None,\n",
      " 'regexp': PosixPath('/work/data/parice/train/regexp.is'),\n",
      " 'shuffle': None,\n",
      " 'tok': None,\n",
      " 'translated_en_is': None,\n",
      " 'translated_is_en': None}\n"
     ]
    }
   ],
   "source": [
    "def get_modifier(modifiers):\n",
    "    if isinstance(modifiers, str):\n",
    "        return modifiers\n",
    "    return '-'.join(modifiers)\n",
    "# List of stages in processing\n",
    "CAT = 'cat'\n",
    "SHUFFLE = 'shuffle'\n",
    "REGEXP = 'regexp'\n",
    "SENT_FIX = 'sent_fix'\n",
    "LOWER = 'lower'\n",
    "TOKENIZE = 'tok'\n",
    "PLACEHOLDERS = 'placeholders'\n",
    "LENGTH = 'length'\n",
    "DROP = 'drop'\n",
    "LM = 'lm-blm'\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "VAL = 'val'\n",
    "BPE = 'bpe'\n",
    "KVISTUR = 'kvistur'\n",
    "FINAL = 'final'\n",
    "TRANSLATED_EN_IS = 'translated_en_is'\n",
    "TRANSLATED_IS_EN = 'translated_is_en'\n",
    "\n",
    "parice_dir = data_dir.joinpath('parice')\n",
    "rmh_dir = data_dir.joinpath('risamalheild')\n",
    "train_parice_dir = parice_dir.joinpath('train')\n",
    "test_parice_dir = parice_dir.joinpath('test')\n",
    "val_parice_dir = parice_dir.joinpath('val')\n",
    "\n",
    "!mkdir -p {train_parice_dir}\n",
    "!mkdir -p {test_parice_dir}\n",
    "!mkdir -p {val_parice_dir}\n",
    "\n",
    "pipeline = [\n",
    "    SHUFFLE,\n",
    "    LOWER, \n",
    "    REGEXP, \n",
    "    TOKENIZE,\n",
    "    PLACEHOLDERS,\n",
    "    LENGTH,\n",
    "    get_modifier((KVISTUR, FINAL)),\n",
    "    get_modifier((BPE, FINAL)),\n",
    "    get_modifier((KVISTUR, LENGTH)),\n",
    "    get_modifier((BPE, LENGTH)),\n",
    "    get_modifier((KVISTUR, LM)),\n",
    "    get_modifier((BPE, LM)),\n",
    "    LM,\n",
    "    KVISTUR,\n",
    "    BPE,\n",
    "    FINAL,\n",
    "    DROP,\n",
    "    TRANSLATED_EN_IS,\n",
    "    TRANSLATED_IS_EN\n",
    "]\n",
    "rmh_stages = [\n",
    "    SENT_FIX,\n",
    "    LOWER,\n",
    "    REGEXP,\n",
    "    TOKENIZE,\n",
    "    PLACEHOLDERS,\n",
    "    KVISTUR,\n",
    "    get_modifier((KVISTUR, FINAL)),\n",
    "    get_modifier((BPE, FINAL)),\n",
    "    get_modifier((KVISTUR, LM)),\n",
    "    get_modifier((BPE, LM)),\n",
    "    get_modifier((KVISTUR, CAT)),\n",
    "    get_modifier((BPE, CAT)),\n",
    "    BPE,\n",
    "    LM,\n",
    "    FINAL,\n",
    "    CAT\n",
    "]\n",
    "parice_pipeline = [\n",
    "    CAT,\n",
    "    SENT_FIX,\n",
    "    SHUFFLE\n",
    "]\n",
    "\n",
    "# If we are not starting from scratch - we try to load all intermediary stages\n",
    "en_parice = c.pipeline_load(parice_dir, parice_pipeline, c.Lang.EN)\n",
    "is_parice = c.pipeline_load(parice_dir, parice_pipeline, c.Lang.IS)\n",
    "en_train = c.pipeline_load(train_parice_dir, pipeline, c.Lang.EN)\n",
    "is_train = c.pipeline_load(train_parice_dir, pipeline, c.Lang.IS)\n",
    "en_test = c.pipeline_load(test_parice_dir, pipeline, c.Lang.EN)\n",
    "is_test = c.pipeline_load(test_parice_dir, pipeline, c.Lang.IS)\n",
    "en_val = c.pipeline_load(val_parice_dir, pipeline, c.Lang.EN)\n",
    "is_val = c.pipeline_load(val_parice_dir, pipeline, c.Lang.IS)\n",
    "rmh = c.pipeline_load(rmh_dir, rmh_stages, c.Lang.IS)\n",
    "pprint(rmh)\n",
    "pprint(en_train)\n",
    "pprint(is_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stytta þjálfunarsetningar\n",
    "Moses á erfitt með að samstilla langar setningar. Við styttum þjálfunarsetningarnar svo einungis setningar sem eru eitt orð eða lengri upp að tölunni sem er skilgreint að neðan. Við höfum tekið eftir því að niðurstöðurnar sem við fáum með hámarkslengd (100) gefa ekki góðar niðurstöður.\n",
    "\n",
    "Þar sem við notum fall sem er skilgreint í Moses og tekur inn tvær skrár í einu fer nafnavenjan eitthvað á flakk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perl: warning: Setting locale failed.\n",
      "perl: warning: Please check that your locale settings:\n",
      "\tLANGUAGE = \"en_US:en\",\n",
      "\tLC_ALL = (unset),\n",
      "\tLC_CTYPE = \"C.UTF-8\",\n",
      "\tLANG = \"en_US.UTF-8\"\n",
      "    are supported and installed on your system.\n",
      "perl: warning: Falling back to the standard locale (\"C\").\n",
      "clean-corpus.perl: processing /work/data/parice/train/bpe-final.en & .is to /work/data/parice/train/bpe-length, cutoff 1-70, ratio 9\n",
      "..........(100000)..........(200000)..........(300000)..........(400000)..........(500000)..........(600000)..........(700000)..........(800000)..........(900000)..........(1000000)..........(1100000)..........(1200000)..........(1300000)..........(1400000)..........(1500000)..........(1600000)..........(1700000)..........(1800000)..........(1900000)..........(2000000)..........(2100000)..........(2200000)..........(2300000)..........(2400000)..........(2500000)..........(2600000)..........(2700000)..........(2800000)..........(2900000)..........(3000000)..........(3100000)..........(3200000)..........(3300000).......\n",
      "Input sentences: 3378149  Output sentences:  3323068\n"
     ]
    }
   ],
   "source": [
    "def corpus_shorten(path, path_out, lang_id_1, lang_id_2, min_length, max_length):\n",
    "    !{os.getenv('MOSESDECODER')}/scripts/training/clean-corpus-n.perl {path} {lang_id_1} {lang_id_2} {path_out} {min_length} {max_length}\n",
    "    return True\n",
    "\n",
    "IN = get_modifier((BPE, FINAL))\n",
    "OUT = get_modifier((BPE, LENGTH))\n",
    "\n",
    "path_out = is_train[FINAL].with_name(OUT)\n",
    "path = is_train[FINAL].parent.joinpath(IN)\n",
    "corpus_shorten(path, path_out, 'en', 'is', 1, 70)\n",
    "\n",
    "is_train[OUT] = is_train[FINAL].with_name(OUT).with_suffix('.is')\n",
    "en_train[OUT] = en_train[FINAL].with_name(OUT).with_suffix('.en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tungumála módel\n",
    "Við búum til KenLM mállíkan til þess að gefa okkur líkindi setninga. Til að flýta uppflettingum þá tungumála módelið samtímis kjörsniðið."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lm(path, out_path, order):\n",
    "    tmp_arpa = c.corpus_create_path(path, 'arpa')\n",
    "    !{os.getenv('MOSESDECODER')}/bin/lmplz --order {order} --temp_prefix {data_dir}/ --memory 50% --discount_fallback < {path} > {tmp_arpa}\n",
    "    !{os.getenv('MOSESDECODER')}/bin/build_binary -S 50% {tmp_arpa} {out_path}\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/data/parice/train/final.is\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 45111359 types 557672\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:6692064 2:23443517440 3:43956596736\n",
      "Statistics:\n",
      "1 557672 D1=0.654933 D2=1.04977 D3+=1.39709\n",
      "2 5194483 D1=0.744091 D2=1.10969 D3+=1.42352\n",
      "3 13501935 D1=0.704238 D2=1.17861 D3+=1.46824\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 364 assuming -p 1.5\n",
      "probing 396 assuming -r models -p 1.5\n",
      "trie    161 without quantization\n",
      "trie     94 assuming -q 8 -b 8 quantization \n",
      "trie    151 assuming -a 22 array pointer compression\n",
      "trie     85 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:6692064 2:83111728 3:270038700\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:6692064 2:83111728 3:270038700\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:66003592 kB\tVmRSS:20764 kB\tRSSMax:15480892 kB\tuser:28.9722\tsys:10.5186\tCPU:39.4908\treal:36.3941\n",
      "Reading /work/data/parice/train/arpa.is\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/data/parice/train/final.en\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 48291472 types 329669\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:3956028 2:23444467712 3:43958378496\n",
      "Statistics:\n",
      "1 329669 D1=0.716064 D2=0.994607 D3+=1.26719\n",
      "2 3577181 D1=0.725363 D2=1.07465 D3+=1.36989\n",
      "3 11255547 D1=0.680109 D2=1.14927 D3+=1.44098\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 283 assuming -p 1.5\n",
      "probing 304 assuming -r models -p 1.5\n",
      "trie    119 without quantization\n",
      "trie     68 assuming -q 8 -b 8 quantization \n",
      "trie    113 assuming -a 22 array pointer compression\n",
      "trie     62 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:3956028 2:57234896 3:225110940\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:3956028 2:57234896 3:225110940\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:65987200 kB\tVmRSS:14152 kB\tRSSMax:15438912 kB\tuser:25.7362\tsys:9.53995\tCPU:35.2762\treal:33.234\n",
      "Reading /work/data/parice/train/arpa.en\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_train[LM] = c.corpus_create_path(is_train[FINAL], LM)\n",
    "en_train[LM] = c.corpus_create_path(en_train[FINAL], LM)\n",
    "\n",
    "create_lm(is_train[FINAL], is_train[LM], order=3)\n",
    "create_lm(en_train[FINAL], en_train[LM], order=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE EN mállíkan\n",
    "Við þurfum að gera BPE mállíkan fyrir ensku. Við notum RMH+train fyrir IS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/data/parice/train/bpe-final.en\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 51851575 types 29977\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:359724 2:23445719040 3:43960725504\n",
      "Statistics:\n",
      "1 29977 D1=0.405575 D2=0.967282 D3+=1.45389\n",
      "2 2821688 D1=0.67773 D2=1.06091 D3+=1.3985\n",
      "3 11237289 D1=0.659857 D2=1.14683 D3+=1.46034\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 258 assuming -p 1.5\n",
      "probing 274 assuming -r models -p 1.5\n",
      "trie     96 without quantization\n",
      "trie     50 assuming -q 8 -b 8 quantization \n",
      "trie     91 assuming -a 22 array pointer compression\n",
      "trie     44 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:359724 2:45147008 3:224745780\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:359724 2:45147008 3:224745780\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:65987200 kB\tVmRSS:6976 kB\tRSSMax:15435332 kB\tuser:24.9196\tsys:8.64991\tCPU:33.5695\treal:32.5489\n",
      "Reading /work/data/parice/train/arpa.en\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IN = get_modifier((BPE, FINAL))\n",
    "OUT = get_modifier((BPE, LM))\n",
    "\n",
    "en_train[OUT] = c.corpus_create_path(en_train[IN], OUT)\n",
    "\n",
    "create_lm(en_train[IN], en_train[OUT], order=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE IS mállíkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IN = get_modifier((BPE, FINAL))\n",
    "OUT = get_modifier((BPE, CAT))\n",
    "\n",
    "rmh[OUT] = c.corpus_create_path(rmh[IN], OUT)\n",
    "c.corpora_combine((is_train[IN], rmh[IN]), rmh[OUT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/data/risamalheild/bpe-cat.is\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 1898165984 types 35391\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:424692 2:23445696512 3:43960680448\n",
      "Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 35391 D1=0.5 D2=1 D3+=1.5\n",
      "2 27682187 D1=0.627567 D2=1.08264 D3+=1.49348\n",
      "3 236858317 D1=0.640482 D2=1.21813 D3+=1.5375\n",
      "Memory estimate for binary LM:\n",
      "type      MB\n",
      "probing 4700 assuming -p 1.5\n",
      "probing 4858 assuming -r models -p 1.5\n",
      "trie    1680 without quantization\n",
      "trie     876 assuming -q 8 -b 8 quantization \n",
      "trie    1621 assuming -a 22 array pointer compression\n",
      "trie     817 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:424692 2:442914992 3:4737166340\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:424692 2:442914992 3:4737166340\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:65987200 kB\tVmRSS:6984 kB\tRSSMax:19842952 kB\tuser:787.76\tsys:68.9082\tCPU:856.669\treal:839.076\n",
      "Reading /work/data/risamalheild/arpa.is\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IN = get_modifier((BPE, CAT))\n",
    "OUT = get_modifier((BPE, LM))\n",
    "\n",
    "rmh[OUT] = c.corpus_create_path(rmh[IN], OUT)\n",
    "\n",
    "create_lm(rmh[IN], rmh[OUT], order=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kvistur IS mállíkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IN = get_modifier((KVISTUR, FINAL))\n",
    "OUT = get_modifier((KVISTUR, CAT))\n",
    "\n",
    "rmh[OUT] = c.corpus_create_path(rmh[IN], OUT)\n",
    "c.corpora_combine((is_train[IN], rmh[IN]), rmh[OUT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/data/risamalheild/kvistur-cat.is\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 1686354880 types 2865258\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:34383096 2:32812222464 3:61522919424\n"
     ]
    }
   ],
   "source": [
    "IN = get_modifier((KVISTUR, CAT))\n",
    "OUT = get_modifier((KVISTUR, LM))\n",
    "\n",
    "rmh[OUT] = c.corpus_create_path(rmh[IN], OUT)\n",
    "\n",
    "create_lm(rmh[IN], rmh[OUT], order=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sameina RMH og IS ParIce fyrir mállíkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IN = get_modifier((FINAL))\n",
    "OUT = get_modifier((CAT))\n",
    "\n",
    "rmh[OUT] = c.corpus_create_path(rmh[IN], OUT)\n",
    "c.corpora_combine((is_train[IN], rmh[IN]), rmh[OUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búa til mállíkan að lengd 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/data/risamalheild/cat.is\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 1459492635 types 5833046\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:69996552 2:11461582848 3:21490468864 4:34384748544\n",
      "Statistics:\n",
      "1 5833046 D1=0.702775 D2=1.03479 D3+=1.32363\n",
      "2 84061923 D1=0.746733 D2=1.07179 D3+=1.34939\n",
      "3 332835285 D1=0.814597 D2=1.13232 D3+=1.34811\n",
      "4 629424197 D1=0.735924 D2=1.39628 D3+=1.56914\n",
      "Memory estimate for binary LM:\n",
      "type       MB\n",
      "probing 20491 assuming -p 1.5\n",
      "probing 22899 assuming -r models -p 1.5\n",
      "trie     9940 without quantization\n",
      "trie     5878 assuming -q 8 -b 8 quantization \n",
      "trie     8885 assuming -a 22 array pointer compression\n",
      "trie     4823 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:69996552 2:1344990768 3:6656705700 4:15106180728\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:69996552 2:1344990768 3:6656705700 4:15106180728\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "Name:lmplz\tVmPeak:66114456 kB\tVmRSS:6160 kB\tRSSMax:28208648 kB\tuser:1469.73\tsys:261.073\tCPU:1730.8\treal:1473.02\n",
      "Reading /work/data/risamalheild/arpa.is\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IN = get_modifier((CAT))\n",
    "OUT = get_modifier((LM, '4'))\n",
    "\n",
    "rmh[OUT] = c.corpus_create_path(rmh[IN], OUT)\n",
    "\n",
    "create_lm(rmh[IN], rmh[OUT], order=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prófa tungumála módel, það ættu ekki að vera nein óþekkt orð."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "þetta=408 2 -1.7515687\ter=108 3 -0.45247617\tflott=6918 4 -3.1981769\tíslensk=8107 2 -4.3185043\tsetning=37795 2 -5.0770183\t,=25 2 -1.4574796\ter=108 3 -2.1485984\tþað=260 3 -1.7368271\tekki=184 4 -0.7310319\t?=97 4 -0.9805836\t</s>=2 4 -0.06486414\tTotal: -21.917128 OOV: 0\n",
      "Perplexity including OOVs:\t98.28022595608707\n",
      "Perplexity excluding OOVs:\t98.28022595608707\n",
      "OOVs:\t0\n",
      "Tokens:\t11\n",
      "Name:query\tVmPeak:21003588 kB\tVmRSS:4760 kB\tRSSMax:20987992 kB\tuser:0\tsys:26.9743\tCPU:26.9743\treal:26.9732\n",
      "this=195 2 -1.8074161\tis=188 3 -0.68361896\ta=12 3 -1.0045757\tnice=1048 3 -2.8550868\tenglish=6319 1 -4.6239047\tsentence=2958 1 -5.020405\t,=6 2 -1.1387969\tright=170 2 -3.7610703\t?=94 3 -0.14322345\t</s>=2 3 -0.034358077\tTotal: -21.072456 OOV: 0\n",
      "Perplexity including OOVs:\t128.0105124034037\n",
      "Perplexity excluding OOVs:\t128.0105124034037\n",
      "OOVs:\t0\n",
      "Tokens:\t10\n",
      "Name:query\tVmPeak:310420 kB\tVmRSS:4876 kB\tRSSMax:294940 kB\tuser:0\tsys:0.392519\tCPU:0.392519\treal:0.399088\n"
     ]
    }
   ],
   "source": [
    "def eval_sentence(lm_model, sentence):\n",
    "   !echo \"{sentence}\" | {os.getenv('MOSESDECODER')}/bin/query {lm_model}\n",
    "\n",
    "eval_sentence(rmh[get_modifier((LM, '4'))], \"þetta er flott íslensk setning , er það ekki ?\")\n",
    "eval_sentence(en_train[LM], \"this is a nice english sentence , right ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moses þjálfunar föll\n",
    "Næstu föll snúa að þjálfun Moses og annarra atriða sem þarf að hafa í huga. Þjálfunin tekur um 12 klst.\n",
    "Til þess að sjá framgang þjálfunar - sjá útprent þegar kallað er í föllin. Síðasta skrefið metur þýðingar Moses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_moses(model_dir, corpus, lang_from, lang_to, lang_to_lm, lm_order):\n",
    "    print(f'tail -f {model_dir}/training.out')\n",
    "    result = !{os.getenv('MOSESDECODER')}/scripts/training/train-model.perl -root-dir {model_dir} \\\n",
    "        -corpus {corpus} \\\n",
    "        -f {lang_from} -e {lang_to} \\\n",
    "        -alignment grow-diag-final-and -reordering msd-bidirectional-fe \\\n",
    "        -lm 0:{lm_order}:{lang_to_lm}:8 \\\n",
    "        -mgiza -mgiza-cpus {os.getenv('THREADS')} \\\n",
    "        -cores {os.getenv('THREADS')} \\\n",
    "        -external-bin-dir {os.getenv('MOSESDECODER_TOOLS')} &> {model_dir}/training.out\n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_moses(model_dir, corpus_val_from, corpus_val_to, base_moses_ini):\n",
    "    print(f'tail -f {model_dir}/tune.out')\n",
    "    result = !{os.getenv('MOSESDECODER')}/scripts/training/mert-moses.pl \\\n",
    "        {corpus_val_from} \\\n",
    "        {corpus_val_to} \\\n",
    "        {os.getenv('MOSESDECODER')}/bin/moses {base_moses_ini} \\\n",
    "        --mertdir {os.getenv('MOSESDECODER')}/bin \\\n",
    "        --working-dir {model_dir} \\\n",
    "        --decoder-flags=\"-threads {os.getenv('THREADS')}\" &> {model_dir}/tune.out\n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_binarisation(tuned_moses_ini,\n",
    "                         lm_path_in,\n",
    "                         lm_path_out,\n",
    "                         binarised_moses_ini,\n",
    "                         binarised_phrase_table,\n",
    "                         binarised_reordering_table):\n",
    "    !cp {tuned_moses_ini} {binarised_moses_ini}\n",
    "    !cp {lm_path_in} {lm_path_out}\n",
    "    # Adjust the path in the moses.ini file to point to the new files.\n",
    "    escaped_path_in = str(lm_path_in).replace(r'/', '\\/')\n",
    "    escaped_path_out = str(lm_path_out).replace(r'/', '\\/')\n",
    "    !sed -i 's/{escaped_path_in}/{escaped_path_out}/' {binarised_moses_ini}\n",
    "    # Adjust the path in the moses.ini file to point to the new files.\n",
    "    escaped_path = str(binarised_phrase_table).replace(r'/', '\\/')\n",
    "    !sed -i 's/PhraseDictionaryMemory/PhraseDictionaryCompact/' {binarised_moses_ini}\n",
    "    !sed -i 's/4 path=.*\\.gz input-factor/4 path={escaped_path} input-factor/' {binarised_moses_ini}\n",
    "    # Adjust the path in the moses.ini file\n",
    "    escaped_path = str(binarised_reordering_table).replace(r'/', '\\/')\n",
    "    !sed -i 's/0 path=.*\\.gz$/0 path={escaped_path}/' {binarised_moses_ini}\n",
    "    \n",
    "def binarise_phrase_table(base_phrase_table, binarised_phrase_table):\n",
    "    #Create the table\n",
    "    !{os.getenv('MOSESDECODER')}/bin/processPhraseTableMin \\\n",
    "        -in {base_phrase_table} \\\n",
    "        -nscores 4 \\\n",
    "        -out {binarised_phrase_table}\n",
    "    \n",
    "def binarise_reordering_table(base_reordering_table, binarised_reordering_table):\n",
    "    #Create the table\n",
    "    !{os.getenv('MOSESDECODER')}/bin/processLexicalTableMin \\\n",
    "        -in {base_reordering_table} \\\n",
    "        -out {binarised_reordering_table}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It only makes sense to filter the model when you know what text the system needs to translate.\n",
    "def filter_model(out_dir, moses_ini, corpus):\n",
    "    !{os.getenv('MOSESDECODER')}/scripts/training/filter-model-given-input.pl {out_dir} {moses_ini} {corpus}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_corpus(moses_ini, corpus, corpus_translated):\n",
    "    !{os.getenv('MOSESDECODER')}/bin/moses \\\n",
    "        -f {moses_ini} < {corpus} > {corpus_translated}\n",
    "    \n",
    "def eval_translation(corpus_gold, corpus_translated):\n",
    "    result = !{os.getenv('MOSESDECODER')}/scripts/generic/multi-bleu.perl -lc {corpus_gold} < {corpus_translated}\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byrja þjálfanir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune_eval(LM,\n",
    "                    LM_ORDER,\n",
    "                    FROM,\n",
    "                    TO,\n",
    "                    MODIFIER,\n",
    "                    TRAIN_IN,\n",
    "                    VAL_IN,\n",
    "                    VAL_OUT,\n",
    "                    TEST_IN,\n",
    "                    TEST_OUT):\n",
    "    model_dir = working_dir.joinpath(f'{FROM}-{TO}-{MODIFIER}')\n",
    "    base_model_dir = model_dir.joinpath('base')\n",
    "    tuned_model_dir = model_dir.joinpath('tuned')\n",
    "    binarised_model_dir = model_dir.joinpath('binarised')\n",
    "    !mkdir -p {base_model_dir}\n",
    "    !mkdir -p {tuned_model_dir}\n",
    "    !mkdir -p {binarised_model_dir}\n",
    "\n",
    "    base_moses_ini = base_model_dir.joinpath('model/moses.ini')\n",
    "    base_phrase_table = base_model_dir.joinpath('model/phrase-table.gz')\n",
    "    base_reordering_table = base_model_dir.joinpath('model/reordering-table.wbe-msd-bidirectional-fe.gz')\n",
    "\n",
    "    tuned_moses_ini = tuned_model_dir.joinpath('moses.ini')\n",
    "\n",
    "    binarised_moses_ini = binarised_model_dir.joinpath('moses.ini')\n",
    "    binarised_phrase_table = binarised_model_dir.joinpath('phrase-table')\n",
    "    binarised_reordering_table = binarised_model_dir.joinpath('reordering-table')\n",
    "\n",
    "    # train\n",
    "    train_moses(base_model_dir, TRAIN_IN, FROM, TO, LM, lm_order=LM_ORDER)\n",
    "\n",
    "    # tune\n",
    "    tune_moses(tuned_model_dir, VAL_IN, VAL_OUT, base_moses_ini)\n",
    "\n",
    "    # binarise\n",
    "    !mkdir -p {binarised_model_dir}\n",
    "\n",
    "    lm_out = binarised_model_dir.joinpath('lm.blm')\n",
    "\n",
    "    prepare_binarisation(tuned_moses_ini, \n",
    "                         LM,\n",
    "                         lm_out, \n",
    "                         binarised_moses_ini, \n",
    "                         binarised_phrase_table, \n",
    "                         binarised_reordering_table)\n",
    "    binarise_phrase_table(base_phrase_table, binarised_phrase_table)\n",
    "    binarise_reordering_table(base_reordering_table, binarised_reordering_table)\n",
    "\n",
    "    # translate\n",
    "    translated = binarised_model_dir.joinpath(f'translated.{FROM}')\n",
    "\n",
    "    translate_corpus(binarised_moses_ini, TEST_IN, translated)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMH en-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail -f /work/en-is-rmh-again/base/training.out\n"
     ]
    }
   ],
   "source": [
    "train_tune_eval(LM = rmh[LM],\n",
    "                LM_ORDER = 3,\n",
    "                FROM = 'en',\n",
    "                TO = 'is',\n",
    "                MODIFIER = 'rmh-again',\n",
    "                TRAIN_IN = is_train[FINAL].parent.joinpath(get_modifier((LENGTH))),\n",
    "                VAL_IN = en_val[get_modifier((FINAL))],\n",
    "                VAL_OUT = is_val[get_modifier((FINAL))],\n",
    "                TEST_IN = en_test[get_modifier((FINAL))],\n",
    "                TEST_OUT = is_test[get_modifier((FINAL))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMH-4 en-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tune_eval(LM = rmh[get_modifier((LM, '4'))],\n",
    "                LM_ORDER = 4,\n",
    "                FROM = 'en',\n",
    "                TO = 'is',\n",
    "                MODIFIER = 'rmh-4',\n",
    "                TRAIN_IN = is_train[FINAL].parent.joinpath(get_modifier((LENGTH))),\n",
    "                VAL_IN = en_val[get_modifier((FINAL))],\n",
    "                VAL_OUT = is_val[get_modifier((FINAL))],\n",
    "                TEST_IN = en_test[get_modifier((FINAL))],\n",
    "                TEST_OUT = is_test[get_modifier((FINAL))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tune_eval(LM = is_train[get_modifier((LM))],\n",
    "                LM_ORDER = 3,\n",
    "                FROM = 'en',\n",
    "                TO = 'is',\n",
    "                MODIFIER = 'again',\n",
    "                TRAIN_IN = is_train[FINAL].parent.joinpath(get_modifier((LENGTH))),\n",
    "                VAL_IN = en_val[get_modifier((FINAL))],\n",
    "                VAL_OUT = is_val[get_modifier((FINAL))],\n",
    "                TEST_IN = en_test[get_modifier((FINAL))],\n",
    "                TEST_OUT = is_test[get_modifier((FINAL))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tune_eval(LM = en_train[get_modifier((LM))],\n",
    "                LM_ORDER = 3,\n",
    "                FROM = 'is',\n",
    "                TO = 'en',\n",
    "                MODIFIER = 'again',\n",
    "                TRAIN_IN = is_train[FINAL].parent.joinpath(get_modifier((LENGTH))),\n",
    "                VAL_IN = is_val[get_modifier((FINAL))],\n",
    "                VAL_OUT = en_val[get_modifier((FINAL))],\n",
    "                TEST_IN = is_test[get_modifier((FINAL))],\n",
    "                TEST_OUT = en_test[get_modifier((FINAL))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['perl: warning: Setting locale failed.', 'perl: warning: Please check that your locale settings:', '\\tLANGUAGE = \"en_US:en\",', '\\tLC_ALL = (unset),', '\\tLC_CTYPE = \"C.UTF-8\",', '\\tLANG = \"en_US.UTF-8\"', '    are supported and installed on your system.', 'perl: warning: Falling back to the standard locale (\"C\").', 'It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.', 'BLEU = 50.43, 70.7/53.1/44.7/38.6 (BP=1.000, ratio=1.000, hyp_len=37874, ref_len=37857)']\n",
      "is: • 6 km fyrir bifhjól í flokki 2 ( slagrými hreyfils ≥ 150 cc , vmax @lt@ 130 km / klukkustund ) ,\n",
      " en: • 6 km fyrir bifhjól í flokki 2 ( slagrými hreyfils ≥ 150 cc , vmax @lt@ 130 km / klukkustund ) , \n",
      " is: enska aðgerð varðandi sameiginlega fræðslu e - liður 1. málsgrein 5. grein .\n",
      " en: enska aðgerð varðandi sameiginlega fræðslu e - lið 1. málsgrein 5. grein \n",
      " is: mælingar á reykþéttni útblásturslofts við hröðun ( frá hægagangi og upp í marksnúningshraða , án álags ) .\n",
      " en: mælingar á reykþéttni útblásturslofts við hröðun ( frá hægagangi og upp í marksnúningshraða , án ) . \n",
      " is: aðrir tengivagnar\n",
      " en: aðrir eftirvagnar og festivagnar \n",
      " is: ég er með matareitrun .\n",
      " en: ég er með matareitrun . \n",
      " is: perlur á stærð við kókoshnetur .\n",
      " en: perlur , eins og stór og kókoshnetur . \n",
      " is: þetta markmið skal einkum mæla í fjölgun aðildarríkja sem fella inn samræmdar nálganir við gerð viðbúnaðaráætlana sinna .\n",
      " en: þetta markmið skal einkum mæla í fjölgun aðildarríkja að samþætta samræmdar nálganir í hönnun þeirra viðbúnaðaráætlana . \n",
      " is: allir út , við erum í vanda .\n",
      " en: við höfum allir átt við vandamál að stríða . \n",
      " is: við jafnvægi var þéttni sonidegibs í húð 6 - falt hærri en í plasma .\n",
      " en: jafnvægi stig sonidegibs á húð var 6 - falt hærri en í plasma . \n",
      " is: einkennin komu fram allt frá einum degi til nokkurra mánaða eftir að meðferð hófst .\n",
      " en: tíminn sem leið fram að upphafi einkenna var frá einum degi til nokkurra mánaða frá upphafi meðferðar . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_OUT = is_test[get_modifier((FINAL))]\n",
    "FROM = 'en'\n",
    "TO = 'is'\n",
    "MODIFIER = 'rmh-4'\n",
    "model_dir = working_dir.joinpath(f'{FROM}-{TO}-{MODIFIER}')\n",
    "binarised_model_dir = model_dir.joinpath('binarised')\n",
    "translated = binarised_model_dir.joinpath(f'translated.{FROM}')\n",
    "print(eval_translation(TEST_OUT, translated))\n",
    "print(*c.corpora_peek((TEST_OUT, translated)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To correct comparison we need to map the translated BPE text to the normal test and compare with `test/final.en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_detokenize_bpe(sentence):\n",
    "    pieces = sentence.split(\" \")\n",
    "    return ''.join(pieces).replace('▁', ' ').strip()\n",
    "\n",
    "def corpus_detokenize_bpe(path, out_path):\n",
    "    with path.open() as f_in, out_path.open('w+') as f_out:\n",
    "        for line in f_in:\n",
    "            f_out.write(sent_detokenize_bpe(line)+'\\n')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_detokenized = c.corpus_create_path(translated, 'translated_detokenized')\n",
    "corpus_detokenize_bpe(translated, translated_detokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['perl: warning: Setting locale failed.', 'perl: warning: Please check that your locale settings:', '\\tLANGUAGE = \"en_US:en\",', '\\tLC_ALL = (unset),', '\\tLC_CTYPE = \"C.UTF-8\",', '\\tLANG = \"en_US.UTF-8\"', '    are supported and installed on your system.', 'perl: warning: Falling back to the standard locale (\"C\").', 'It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.', 'BLEU = 47.69, 68.9/50.6/41.8/35.4 (BP=1.000, ratio=1.015, hyp_len=38406, ref_len=37857)']\n",
      "is: • 6 km fyrir bifhjól í flokki 2 ( slagrými hreyfils ≥ 150 cc , vmax @lt@ 130 km / klukkustund ) ,\n",
      " en: • 6 km fyrir bifhjól í flokki 2 ( slagrými hreyfils ≥ 150 cc , vmax @lt@ 130 km / klukkustund ) .\n",
      " is: enska aðgerð varðandi sameiginlega fræðslu e - liður 1. málsgrein 5. grein .\n",
      " en: enska aðgerð varðandi sameiginlega fræðslu e - lið 1. málsgrein 5. grein\n",
      " is: mælingar á reykþéttni útblásturslofts við hröðun ( frá hægagangi og upp í marksnúningshraða , án álags ) .\n",
      " en: mælingar á reykþéttni útblásturslofts við hröðun ( frá hægagangi án álags upp .\n",
      " is: aðrir tengivagnar\n",
      " en: aðrir eftirvagnar og festivagnar\n",
      " is: ég er með matareitrun .\n",
      " en: ég er með matareitrun .\n",
      " is: perlur á stærð við kókoshnetur .\n",
      " en: perlur á stærð við kókoshnetur .\n",
      " is: þetta markmið skal einkum mæla í fjölgun aðildarríkja sem fella inn samræmdar nálganir við gerð viðbúnaðaráætlana sinna .\n",
      " en: þetta markmið skal einkum mæla í fjölgun aðildarríkja samþætta samræmdar nálganir á viðbúnaðaráætlana þeirra .\n",
      " is: allir út , við erum í vanda .\n",
      " en: við höfum allir átt við vandamál að stríða .\n",
      " is: við jafnvægi var þéttni sonidegibs í húð 6 - falt hærri en í plasma .\n",
      " en: jafnvægi sonidegibs á vettvangi sprakk fyrir 6 - falt hærri en í plasma .\n",
      " is: einkennin komu fram allt frá einum degi til nokkurra mánaða eftir að meðferð hófst .\n",
      " en: tími þar til einkenna var frá einum degi eða nokkrum mánuðum eftir að meðferð hófst .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(eval_translation(TEST_OUT, translated_detokenized))\n",
    "print(*c.corpora_peek((TEST_OUT, translated_detokenized)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo\n",
    "Þýða einhvern texta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_en_is(moses_ini, sentence):\n",
    "    sentence = c.sent_process_v1(sentence, c.Lang.EN)\n",
    "    !echo \"{sentence}\" | {os.getenv('MOSESDECODER')}/bin/moses -f {moses_ini}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined parameters (per moses.ini or switch):\n",
      "\tconfig: /work/en-is-rmh/binarised/moses.ini \n",
      "\tdistortion-limit: 6 \n",
      "\tfeature: UnknownWordPenalty WordPenalty PhrasePenalty PhraseDictionaryCompact name=TranslationModel0 num-features=4 path=/work/en-is-rmh/binarised/phrase-table input-factor=0 output-factor=0 LexicalReordering name=LexicalReordering0 num-features=6 type=wbe-msd-bidirectional-fe-allff input-factor=0 output-factor=0 path=/work/en-is-rmh/binarised/reordering-table Distortion KENLM name=LM0 factor=0 path=/work/en-is-rmh/binarised/lm.blm order=3 \n",
      "\tinput-factors: 0 \n",
      "\tmapping: 0 T 0 \n",
      "\tthreads: 10 \n",
      "\tweight: LexicalReordering0= 0.0169919 0.229923 0.142056 0.00709025 -0.0357982 0.136943 Distortion0= -0.0411258 LM0= 0.0329771 WordPenalty0= -0.117491 PhrasePenalty0= -0.0976637 TranslationModel0= 0.0113354 0.00836338 0.1168 0.0054414 UnknownWordPenalty0= 1 \n",
      "line=UnknownWordPenalty\n",
      "FeatureFunction: UnknownWordPenalty0 start: 0 end: 0\n",
      "line=WordPenalty\n",
      "FeatureFunction: WordPenalty0 start: 1 end: 1\n",
      "line=PhrasePenalty\n",
      "FeatureFunction: PhrasePenalty0 start: 2 end: 2\n",
      "line=PhraseDictionaryCompact name=TranslationModel0 num-features=4 path=/work/en-is-rmh/binarised/phrase-table input-factor=0 output-factor=0\n",
      "FeatureFunction: TranslationModel0 start: 3 end: 6\n",
      "line=LexicalReordering name=LexicalReordering0 num-features=6 type=wbe-msd-bidirectional-fe-allff input-factor=0 output-factor=0 path=/work/en-is-rmh/binarised/reordering-table\n",
      "Initializing Lexical Reordering Feature..\n",
      "FeatureFunction: LexicalReordering0 start: 7 end: 12\n",
      "line=Distortion\n",
      "FeatureFunction: Distortion0 start: 13 end: 13\n",
      "line=KENLM name=LM0 factor=0 path=/work/en-is-rmh/binarised/lm.blm order=3\n",
      "FeatureFunction: LM0 start: 14 end: 14\n",
      "Loading UnknownWordPenalty0\n",
      "Loading WordPenalty0\n",
      "Loading PhrasePenalty0\n",
      "Loading LexicalReordering0\n",
      "Loading Distortion0\n",
      "Loading LM0\n",
      "Loading TranslationModel0\n",
      "Created input-output object : [5.572] seconds\n",
      "Translating: \n",
      "Line 1: Initialize search took 0.000 seconds total\n",
      "Line 1: Collecting options took 0.000 seconds at moses/Manager.cpp Line 141\n",
      "Line 1: Search took 0.000 seconds\n",
      "Line 1: Decision rule took 0.000 seconds total\n",
      "Line 1: Additional reporting took 0.000 seconds totalTranslating: this is a proper english \n",
      "Line 1: Translation took 0.001 seconds total\n",
      "sentence , and we can have learnt a better phrase model \n",
      "Line 0: Initialize search took 0.001 seconds total\n",
      "Line 0: Collecting options took 1.093 seconds at moses/Manager.cpp Line 141\n",
      "Line 0: Search took 0.204 seconds\n",
      "þetta er enskan setningunni og við getum hafa lært betri setning fyrirmynd \n",
      "BEST TRANSLATION: þetta er enskan setningunni og við getum hafa lært betri setning fyrirmynd [1111111111111111]  [total=-4.199] core=(0.000,-12.000,8.000,-16.979,-27.442,-6.050,-24.238,-1.645,0.000,0.000,-1.874,0.000,0.000,0.000,-106.908)  \n",
      "\n",
      "BEST TRANSLATION: []  [total=0.000] core=(0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000)  \n",
      "Line 0: Decision rule took 0.000 seconds total\n",
      "Line 0: Additional reporting took 0.000 seconds total\n",
      "Line 0: Translation took 1.298 seconds total\n",
      "Name:moses\tVmPeak:10184564 kB\tVmRSS:945952 kB\tRSSMax:9128312 kB\tuser:5.362\tsys:1.775\tCPU:7.137\treal:7.161\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This is a proper English sentence, and we can have learnt a better phrase model\"\n",
    "print(translate_en_is(binarised_model_dir.joinpath('moses.ini'), sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_is_en(moses_ini, sentence):\n",
    "    sentence = c.sent_process_v1(sentence, c.Lang.IS)\n",
    "    !echo \"{sentence}\" | {os.getenv('MOSESDECODER')}/bin/moses -f {moses_ini}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined parameters (per moses.ini or switch):\n",
      "\tconfig: /work/is-en/binarised/moses.ini \n",
      "\tdistortion-limit: 6 \n",
      "\tfeature: UnknownWordPenalty WordPenalty PhrasePenalty PhraseDictionaryCompact name=TranslationModel0 num-features=4 path=/work/is-en/binarised/phrase-table input-factor=0 output-factor=0 LexicalReordering name=LexicalReordering0 num-features=6 type=wbe-msd-bidirectional-fe-allff input-factor=0 output-factor=0 path=/work/is-en/binarised/reordering-table Distortion KENLM name=LM0 factor=0 path=/work/is-en/binarised/lm-en.blm order=3 \n",
      "\tinput-factors: 0 \n",
      "\tmapping: 0 T 0 \n",
      "\tthreads: 14 \n",
      "\tweight: LexicalReordering0= 0.114192 0.0158818 0.0202684 0.083186 0.0208785 0.197803 Distortion0= 0.0160226 LM0= 0.0632488 WordPenalty0= -0.204654 PhrasePenalty0= -0.0417258 TranslationModel0= 0.0177732 0.00823355 0.188931 0.00720186 UnknownWordPenalty0= 1 \n",
      "line=UnknownWordPenalty\n",
      "FeatureFunction: UnknownWordPenalty0 start: 0 end: 0\n",
      "line=WordPenalty\n",
      "FeatureFunction: WordPenalty0 start: 1 end: 1\n",
      "line=PhrasePenalty\n",
      "FeatureFunction: PhrasePenalty0 start: 2 end: 2\n",
      "line=PhraseDictionaryCompact name=TranslationModel0 num-features=4 path=/work/is-en/binarised/phrase-table input-factor=0 output-factor=0\n",
      "FeatureFunction: TranslationModel0 start: 3 end: 6\n",
      "line=LexicalReordering name=LexicalReordering0 num-features=6 type=wbe-msd-bidirectional-fe-allff input-factor=0 output-factor=0 path=/work/is-en/binarised/reordering-table\n",
      "Initializing Lexical Reordering Feature..\n",
      "FeatureFunction: LexicalReordering0 start: 7 end: 12\n",
      "line=Distortion\n",
      "FeatureFunction: Distortion0 start: 13 end: 13\n",
      "line=KENLM name=LM0 factor=0 path=/work/is-en/binarised/lm-en.blm order=3\n",
      "FeatureFunction: LM0 start: 14 end: 14\n",
      "Loading UnknownWordPenalty0\n",
      "Loading WordPenalty0\n",
      "Loading PhrasePenalty0\n",
      "Loading LexicalReordering0\n",
      "Loading Distortion0\n",
      "Loading LM0\n",
      "Loading TranslationModel0\n",
      "Created input-output object : [1.002] seconds\n",
      "Translating: \n",
      "Line 1: Initialize search took Translating: 0.000 seconds total\n",
      "ég man ekkiLine 1: Collecting options took  eftir0.000 seconds at moses/Manager.cpp Line  neinum141\n",
      " góðum myndum nýlega \n",
      "Line 1: Search took 0.000 seconds\n",
      "Line 0: Initialize search took 0.001 seconds total\n",
      "Line 1: Decision rule took 0.000 seconds total\n",
      "Line 1: Additional reporting took 0.000 seconds total\n",
      "Line 1: Translation took 0.001 seconds total\n",
      "Line 0: Collecting options took 0.292 seconds at moses/Manager.cpp Line 141\n",
      "Line 0: Search took 0.059 seconds\n",
      "i can ' t even remember some good pictures recently \n",
      "BEST TRANSLATION: i can ' t even remember some good pictures recently [11111111]  [total=-2.430] core=(0.000,-10.000,3.000,-3.181,-21.657,-2.845,-23.124,-1.415,0.000,0.000,-1.022,0.000,0.000,0.000,-50.052)  \n",
      "\n",
      "BEST TRANSLATION: []  [total=0.000] core=(0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000,0.000)  \n",
      "Line 0: Decision rule took 0.000 seconds total\n",
      "Line 0: Additional reporting took 0.000 seconds total\n",
      "Line 0: Translation took 0.352 seconds total\n",
      "Name:moses\tVmPeak:2102776 kB\tVmRSS:486448 kB\tRSSMax:979400 kB\tuser:1.074\tsys:0.427\tCPU:1.502\treal:1.498\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Ég man ekki eftir neinum góðum myndum nýlega \"\n",
    "print(translate_is_en(working_dir.joinpath('is-en/binarised').joinpath('moses.ini'), sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
